{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G_rwok8s3nvU"
      },
      "outputs": [],
      "source": [
        "\n",
        "vocab_size= 22000        \n",
        "embedding_dim=32         \n",
        "max_length = 18          \n",
        "trunc_type='post'        \n",
        "padding_type='post'      \n",
        "oov_tok='<OOV>'          # Token à utiliser quand un mot est manquant\n",
        "training_size=22000      # Taille du training Set (pour le split entre train/test)\n",
        "num_epochs = 10          # Nombre epoch pour entrainement du réseau\n",
        "DEL_STOPWORDS = False    # Pour la suppression des Stop Words\n",
        "DO_LEMMATIZE  = True     # Lemmatization des mots\n",
        "PATIENCE_STOPPING = 5    # Patience en nombre d'epoch avant de stopper l'entrainement\n",
        "OPTIMIZER = 'adam'      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tzzBRg-3nvX",
        "outputId": "4aa72772-1579-4268-ecb7-8778831f4a3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les données importées dans Kaggle: \n"
          ]
        }
      ],
      "source": [
        "# Lib Standards\n",
        "import os\n",
        "import io\n",
        "\n",
        "# Désactiver les avertissements\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
        "\n",
        "# Verification de l'import des Data\n",
        "print(\"Les données importées dans Kaggle: \")\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLQkB-I83nvZ",
        "outputId": "7d78c37f-eb90-41a7-c933-5ad912f687bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version de TensorFlow: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "# Import des Librairies Datascience\n",
        "import numpy as np \n",
        "import pandas as pd       \n",
        "import random\n",
        "import nltk\n",
        "\n",
        "# Import TensorFlow et vérification de la version\n",
        "import tensorflow as tf\n",
        "print(\"Version de TensorFlow: {}\".format(tf.__version__))\n",
        "# Version majeure:\n",
        "vers_tf = int((tf.__version__).split(sep='.')[0])\n",
        "\n",
        "if vers_tf < 2:\n",
        "    # eager_execution nécessaire si TensorFlow < 2.0\n",
        "    print('Version < 2 : activation eager execution')\n",
        "    tf.enable_eager_execution()\n",
        "\n",
        "# Import des librairies de NLP\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "# Librairies nltk pour la tokenization et la Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8F_S1-g3nvb",
        "outputId": "ed9c0e1a-845d-4b7d-bb9f-06d25d6bc981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 26709 entries, 0 to 26708\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   article_link  26709 non-null  object\n",
            " 1   headline      26709 non-null  object\n",
            " 2   is_sarcastic  26709 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 626.1+ KB\n"
          ]
        }
      ],
      "source": [
        "# Toujours frâce à la librairie Panda nous procédons à l'ouverture du fichier de data\n",
        "# Ouverture du fichier de données contenant les titres\n",
        "df = pd.read_json(\"/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "\n",
        "# Pour voir le nom des colonnes et la taille des objects du dataset importé\n",
        "# Objet dataframe, nous observons grâce à cette sortie : \n",
        "#     Les colonnes du DataFrame\n",
        "#     leur Type\n",
        "#     Le nombre d'enregistrements\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouvelle section"
      ],
      "metadata": {
        "id": "k2nbm2Da4Nni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRPveOGr4V_c",
        "outputId": "6b9ee999-c800-46af-9bd6-40e7a3bc6978"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cIbE4JbM3nvc",
        "outputId": "a22332a1-f252-49e8-df93-46a48e14247f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        article_link  \\\n",
              "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
              "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
              "2  https://local.theonion.com/mom-starting-to-fea...   \n",
              "3  https://politics.theonion.com/boehner-just-wan...   \n",
              "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
              "\n",
              "                                            headline  is_sarcastic  \n",
              "0  former versace store clerk sues over secret 'b...             0  \n",
              "1  the 'roseanne' revival catches up to our thorn...             0  \n",
              "2  mom starting to fear son's web series closest ...             1  \n",
              "3  boehner just wants wife to listen, not come up...             1  \n",
              "4  j.k. rowling wishes snape happy birthday in th...             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0bea37a8-aea8-4607-b19a-35e800198df3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_link</th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0bea37a8-aea8-4607-b19a-35e800198df3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0bea37a8-aea8-4607-b19a-35e800198df3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0bea37a8-aea8-4607-b19a-35e800198df3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# On examine les premières lignes du dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnPVNZcR3nve",
        "outputId": "9d8edf18-31bf-43df-9935-108b76c2ed36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il y a 11724 titres sarcastiques vs 14985 qui ne le sont pas\n"
          ]
        }
      ],
      "source": [
        "# denombrage du nombre articles classifié sarcastic et des autres \n",
        "nb_sarcastic = (df['is_sarcastic'] == 1).sum()\n",
        "nb_not_sarcastic = (df['is_sarcastic'] == 0).sum()\n",
        "print(\"Il y a {} titres sarcastiques vs {} qui ne le sont pas\".format(nb_sarcastic, nb_not_sarcastic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVgQeJHJ3nvf",
        "outputId": "ad0ea79d-1be2-444f-df26-90cf94af51a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.huffingtonpost.com/entry/drake-restaurant-toronto_us_5a577986e4b0330eab08a67e\n",
            "drake hasn't even opened his restaurant and already threw a party there\n",
            "Sarcasme: 0\n",
            "---\n",
            "https://www.huffingtonpost.com/entry/remarkable-new-documentar_b_6047168.html\n",
            "remarkable new documentary on burma's children\n",
            "Sarcasme: 0\n",
            "---\n",
            "https://www.huffingtonpost.com/entry/beyond-the-classroom-expe_b_6920532.html\n",
            "beyond the classroom: experiencing technology innovation up-close-and-personal at sxsw\n",
            "Sarcasme: 0\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Affichage au hasard de quelques titres et de leur label\n",
        "for i in range(3):\n",
        "    n = random.randrange(len(df['headline']))\n",
        "    print(df['article_link'][n])\n",
        "    print(df['headline'][n])\n",
        "    print(\"Sarcasme: {}\".format(df['is_sarcastic'][n]))\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b5TS9wiJ3nvh"
      },
      "outputs": [],
      "source": [
        "# Contruction des listes de phrases(X) et de labels(y)\n",
        "X=list(df['headline'])\n",
        "y=list(df['is_sarcastic'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3BcZMNTs3nvi",
        "outputId": "3ac81ced-b7cb-4dc4-c59e-849a4d0d9947"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7652c9201360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import des stopwords grace à nltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nous avons {} stopwords pour le language anglais. Exemple:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# Import des stopwords grace à nltk\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "print(\"Nous avons {} stopwords pour le language anglais. Exemple:\".format(len(stopwords)))\n",
        "print(stopwords[0:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUI03BOl3nvj"
      },
      "outputs": [],
      "source": [
        "# Fonction pour gérer la suppression des stopwords\n",
        "def process_stop_words(X, display_res=True):\n",
        "    \n",
        "    if DEL_STOPWORDS:\n",
        "        # Une autre façon de Tokeniser:\n",
        "        # avec fonctions lambda + fonction map\n",
        "        # tokenize = lambda x: text_to_word_sequence(x, \n",
        "        #                                    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
        "        #                                    lower=True, split=' ')\n",
        "        # X_seq = list(map(tokenize, X))\n",
        "        \n",
        "        # avec la liste comprehension et la librairie NLTK:\n",
        "        X_seq = [[word for word in word_tokenize(s)] for s in X]\n",
        "        \n",
        "        # Suppression des Stopwords \n",
        "        X_seq_no_stops = [[word for word in s if word not in stopwords] for s in X_seq]\n",
        "        \n",
        "        if display_res:\n",
        "            print(\"Avant Tokenization:     {}\".format(X[10]))\n",
        "            print(\"Après Tokenization:     {}\".format(X_seq[10]))\n",
        "            print(\"Sans Stopwords:         {}\".format(X_seq_no_stops[10]))\n",
        "        return X_seq_no_stops\n",
        "    else:\n",
        "        print(\"Pas de traitement stopwords\")\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiWychdz3nvk"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------\n",
        "# Gain apporté par les 'list comprehension' de Python\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Grâce a une seule ligne ci-dessous...\n",
        "#[[word for word in s if word not in stopwords] for s in X_seq]\n",
        "\n",
        "# ...factoriser le code suivant:\n",
        "\n",
        "# X_no_stops = [[0] * 0 for i in range(len(X))]\n",
        "# for n,title in enumerate(X):\n",
        "#    for word in title:\n",
        "#        if word not in stopwords:\n",
        "#           X_no_stops[n].append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjPrsj5I3nvk"
      },
      "outputs": [],
      "source": [
        "# Processing des Stop Words\n",
        "X = process_stop_words(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnEYAGU93nvm"
      },
      "outputs": [],
      "source": [
        "print(\"Exemple de Lemme:\")\n",
        "list_lemm_exemple = ['cars', 'tools', 'airplane', 'nicely', 'sarcastic']\n",
        "for w in list_lemm_exemple:\n",
        "    print(\"    {}   =>   {}\".format(w, WordNetLemmatizer().lemmatize(w)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4RCf0Lc3nvn"
      },
      "outputs": [],
      "source": [
        "def process_lemmatization(X, display_res=True):\n",
        "    if DO_LEMMATIZE:\n",
        "        lemm = WordNetLemmatizer()\n",
        "        if display_res:\n",
        "            print(\"Avant Lemmatization :    {}\".format(X[10]))\n",
        "        Xlem = X.copy()\n",
        "\n",
        "        # Si nous avons une liste de liste \n",
        "        # suite à la tokenization des titres pour suppression des stopword\n",
        "        \n",
        "        if DEL_STOPWORDS:\n",
        "            # Ancienne méthode avec parcourt de liste\n",
        "            # for row,title in enumerate(X):\n",
        "            #    for col,word in enumerate(title):\n",
        "            #        Xlem[row][col] = lemm.lemmatize(word)\n",
        "            \n",
        "            # Plus simple Avec les génerateurs de liste :-)\n",
        "            Xlem = [[lemm.lemmatize(word) for word in s] for s in X]\n",
        "\n",
        "        # une liste de titre\n",
        "        else:\n",
        "            Xlem = [[lemm.lemmatize(word) for word in word_tokenize(s)] for s in X]\n",
        "\n",
        "        if display_res:\n",
        "            print(\"Apres Lemmatization :    {}\".format(Xlem[10]))\n",
        "\n",
        "        return Xlem\n",
        "    else:\n",
        "        print(\"Pas de lemmatization demandée\")\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX4FBzg13nvo"
      },
      "outputs": [],
      "source": [
        "# Traitement de la lemmatization\n",
        "X = process_lemmatization(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmE6liX83nvp"
      },
      "outputs": [],
      "source": [
        "# Constitution des jeux d'entrainements et de test\n",
        "X_train = X[:training_size]\n",
        "y_train = y[:training_size]\n",
        "X_test  = X[training_size:]\n",
        "y_test  = y[training_size:]\n",
        "\n",
        "print(\"Taille jeu d'entrainement : {}\".format(len(X_train)))\n",
        "print(\"Taille jeu de test : {}\".format(len(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRjqKNUW3nvq"
      },
      "outputs": [],
      "source": [
        "# Instanciation du Tokenizer avec les paramètres\n",
        "tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\n",
        "# Adaptation du Tokenizer au jeu d'entrainement\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "# Création d'un dictionnaire d'index des mots\n",
        "word_index = tokenizer.word_index                             \n",
        "\n",
        "# Conversion en sequences d'entier\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "# Ajout du padding \n",
        "# Objectif: permettre à chaque séquence de possèder la même longueur\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen = max_length, truncating = trunc_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl5AopcZ3nvr"
      },
      "outputs": [],
      "source": [
        "# Traitement du jeu de test à l'identique du jeu d'entrainement\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen = max_length, truncating = trunc_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "busY2tgZ3nvs"
      },
      "outputs": [],
      "source": [
        "# Nombre de mot dans le corpus\n",
        "print(\"Nombre de mot dans le corpus du jeu d'entrainement : {}\".format(len(word_index)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujg3oUzH3nvs"
      },
      "outputs": [],
      "source": [
        "# Création d'un index inversé des mots / digit associé\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "# Affichage de quelques phrases aléatoires:\n",
        "for i in range(3):\n",
        "    alea = random.randrange(len(X_train))\n",
        "    print(\"phrase originale: {}\".format(X_train[alea]))\n",
        "    print(\"sequence : {}\".format(X_train_sequences[alea]))\n",
        "    print(\"sequence + bourrage: {}\".format(X_train_padded[alea]))\n",
        "    print(\"reconstitution: {}\".format(decode_review(X_train_padded[alea])))\n",
        "    print(\"----------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOff4fL73nvu"
      },
      "outputs": [],
      "source": [
        "# Création du réseau neuronal avec LSTM\n",
        "model = tf.keras.Sequential([\n",
        "    \n",
        "    ### Couche d'entrée\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    \n",
        "    ### Couche des LSTM\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "    \n",
        "    ### Couche de neurones connectés\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    \n",
        "    ### Couche de Sortie\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer=OPTIMIZER, loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLY2u8NZ3nvv"
      },
      "outputs": [],
      "source": [
        "# Callbacks (Expliqué dans les tutoriaux CNN)\n",
        "checkpoint = ModelCheckpoint(\"model_sarcasm.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=PATIENCE_STOPPING, verbose=1, mode='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTxxEajE3nvw"
      },
      "outputs": [],
      "source": [
        "### Entrainement du modèle\n",
        "# NB : il est possible d'inclure le paramètre validation_split dans fit() \n",
        "# afin de ne pas traiter le split train/test plus tôt\n",
        "\n",
        "history=model.fit(X_train_padded,\n",
        "                  y_train,\n",
        "                  epochs=num_epochs,\n",
        "                  validation_data=(X_test_padded, y_test),\n",
        "                  callbacks = [checkpoint, early],\n",
        "                  verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj1iJLOL3nvx"
      },
      "outputs": [],
      "source": [
        "# Graphing de l'apprentissage\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\n",
        "fig.suptitle(\"Performance\")\n",
        "ax1.plot(history.history['acc'])\n",
        "ax1.plot(history.history['val_acc'])\n",
        "vline_cut = np.where(history.history['val_acc'] == np.max(history.history['val_acc']))[0][0]\n",
        "ax1.axvline(x=vline_cut, color='k', linestyle='--')\n",
        "ax1.set_title(\"Model Accuracy\")\n",
        "ax1.legend(['train', 'test'])\n",
        "\n",
        "ax2.plot(history.history['loss'])\n",
        "ax2.plot(history.history['val_loss'])\n",
        "vline_cut = np.where(history.history['val_loss'] == np.min(history.history['val_loss']))[0][0]\n",
        "ax2.axvline(x=vline_cut, color='k', linestyle='--')\n",
        "ax2.set_title(\"Model Loss\")\n",
        "ax2.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD0Kaf7L3nvx"
      },
      "source": [
        "Nous observons avec les graphs qu'après 2 epochs la précision sur le jeu de test diminue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjVyamVz3nvy"
      },
      "outputs": [],
      "source": [
        "# Test sur deux phrases de présence ou nom de saracasme\n",
        "sentence = [\"granny starting to fear spiders in the garden might be real!\", \"Doctor House season finale this sunday evening on TV.\"]\n",
        "sentence = process_stop_words(sentence, display_res=False)\n",
        "sentence = process_lemmatization(sentence, display_res=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYedu2jt3nvz"
      },
      "outputs": [],
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(\"Probabilité de détection du sarcasme: \\n\")\n",
        "print(sentence)\n",
        "print(model.predict(padded))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "sarcasm-detection-tensorflow.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}